{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity=\"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-19 10:58:21 - root - INFO - test info\n",
      "2020-01-19 10:58:21 - root - DEBUG - test debug\n"
     ]
    }
   ],
   "source": [
    "# logging\n",
    "import logging, logging.config\n",
    "logging.getLogger(\"py4j\").setLevel(logging.CRITICAL)\n",
    "logging.config.dictConfig({\n",
    "    'version': 1,\n",
    "    'disable_existing_loggers': False,\n",
    "    'formatters': {\n",
    "        'simple': {\n",
    "            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            'datefmt': '%Y-%m-%d %H:%M:%S',\n",
    "        },\n",
    "    },\n",
    "    'handlers': {\n",
    "        'console': {\n",
    "            'level': 'DEBUG',\n",
    "            'class': 'logging.StreamHandler',\n",
    "            'formatter': 'simple',\n",
    "        }\n",
    "    },\n",
    "    'loggers': {\n",
    "        '': {\n",
    "            'handlers': ['console'],\n",
    "            'level': 'DEBUG',\n",
    "            'propagate': True,\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "logger = logging.getLogger(\"\")\n",
    "logger.info(\"test info\")\n",
    "logger.debug(\"test debug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark import\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.window as W\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "import itertools\n",
    "import traceback\n",
    "from functools import reduce\n",
    "import hashlib\n",
    "import pickle\n",
    "import time\n",
    "import graphviz\n",
    "import pytest\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
